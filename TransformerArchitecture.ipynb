{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96298cca-ac7a-4f4e-905c-acc20e6dec7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a639c816-50d1-43e2-a8bb-7ca5df399f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0+cpu\n",
      "None\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "041ffa67-48bd-4f65-8378-139163d421ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.1+cu118\n",
      "11.8\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69ede2d5-6562-464b-b47b-46d9f2711043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Found GPU: NVIDIA GeForce GTX 1650\n",
      "\n",
      " Downloading/Loading MNIST Data...\n",
      "\n",
      " Starting Training on cuda...\n",
      "----------------------------------------\n",
      "   Epoch 1 | Batch 000/469 | Loss: 2.4996\n",
      "   Epoch 1 | Batch 150/469 | Loss: 1.8654\n",
      "   Epoch 1 | Batch 300/469 | Loss: 1.7765\n",
      "   Epoch 1 | Batch 450/469 | Loss: 1.4664\n",
      "----------------------------------------\n",
      " Epoch 1 Summary:\n",
      "   Time     : 362.7s\n",
      "   Train Acc: 34.09% | Test Acc: 47.28%\n",
      "----------------------------------------\n",
      "\n",
      "   Epoch 2 | Batch 000/469 | Loss: 1.4553\n",
      "   Epoch 2 | Batch 150/469 | Loss: 1.3647\n",
      "   Epoch 2 | Batch 300/469 | Loss: 1.1759\n",
      "   Epoch 2 | Batch 450/469 | Loss: 1.2280\n",
      "----------------------------------------\n",
      " Epoch 2 Summary:\n",
      "   Time     : 147.4s\n",
      "   Train Acc: 55.47% | Test Acc: 60.05%\n",
      "----------------------------------------\n",
      "\n",
      "   Epoch 3 | Batch 000/469 | Loss: 1.1552\n",
      "   Epoch 3 | Batch 150/469 | Loss: 1.1238\n",
      "   Epoch 3 | Batch 300/469 | Loss: 0.8622\n",
      "   Epoch 3 | Batch 450/469 | Loss: 0.8892\n",
      "----------------------------------------\n",
      " Epoch 3 Summary:\n",
      "   Time     : 145.0s\n",
      "   Train Acc: 63.61% | Test Acc: 67.00%\n",
      "----------------------------------------\n",
      "\n",
      "   Epoch 4 | Batch 000/469 | Loss: 0.9591\n",
      "   Epoch 4 | Batch 150/469 | Loss: 0.8656\n",
      "   Epoch 4 | Batch 300/469 | Loss: 0.7408\n",
      "   Epoch 4 | Batch 450/469 | Loss: 0.8043\n",
      "----------------------------------------\n",
      " Epoch 4 Summary:\n",
      "   Time     : 146.0s\n",
      "   Train Acc: 69.14% | Test Acc: 72.70%\n",
      "----------------------------------------\n",
      "\n",
      "   Epoch 5 | Batch 000/469 | Loss: 0.7239\n",
      "   Epoch 5 | Batch 150/469 | Loss: 0.8794\n",
      "   Epoch 5 | Batch 300/469 | Loss: 0.8316\n",
      "   Epoch 5 | Batch 450/469 | Loss: 0.8099\n",
      "----------------------------------------\n",
      " Epoch 5 Summary:\n",
      "   Time     : 146.4s\n",
      "   Train Acc: 73.17% | Test Acc: 74.42%\n",
      "----------------------------------------\n",
      "\n",
      "   Epoch 6 | Batch 000/469 | Loss: 0.6949\n",
      "   Epoch 6 | Batch 150/469 | Loss: 0.7190\n",
      "   Epoch 6 | Batch 300/469 | Loss: 0.7618\n",
      "   Epoch 6 | Batch 450/469 | Loss: 0.6865\n",
      "----------------------------------------\n",
      " Epoch 6 Summary:\n",
      "   Time     : 142.7s\n",
      "   Train Acc: 75.66% | Test Acc: 75.34%\n",
      "----------------------------------------\n",
      "\n",
      "   Epoch 7 | Batch 000/469 | Loss: 0.7939\n",
      "   Epoch 7 | Batch 150/469 | Loss: 0.6179\n",
      "   Epoch 7 | Batch 300/469 | Loss: 0.6482\n",
      "   Epoch 7 | Batch 450/469 | Loss: 0.7319\n",
      "----------------------------------------\n",
      " Epoch 7 Summary:\n",
      "   Time     : 144.6s\n",
      "   Train Acc: 76.77% | Test Acc: 78.98%\n",
      "----------------------------------------\n",
      "\n",
      "   Epoch 8 | Batch 000/469 | Loss: 0.7292\n",
      "   Epoch 8 | Batch 150/469 | Loss: 0.5275\n",
      "   Epoch 8 | Batch 300/469 | Loss: 0.5539\n",
      "   Epoch 8 | Batch 450/469 | Loss: 0.6848\n",
      "----------------------------------------\n",
      " Epoch 8 Summary:\n",
      "   Time     : 143.7s\n",
      "   Train Acc: 77.66% | Test Acc: 74.51%\n",
      "----------------------------------------\n",
      "\n",
      "   Epoch 9 | Batch 000/469 | Loss: 0.7309\n",
      "   Epoch 9 | Batch 150/469 | Loss: 0.5996\n",
      "   Epoch 9 | Batch 300/469 | Loss: 0.4593\n",
      "   Epoch 9 | Batch 450/469 | Loss: 0.4985\n",
      "----------------------------------------\n",
      " Epoch 9 Summary:\n",
      "   Time     : 142.4s\n",
      "   Train Acc: 78.42% | Test Acc: 80.46%\n",
      "----------------------------------------\n",
      "\n",
      "   Epoch 10 | Batch 000/469 | Loss: 0.5429\n",
      "   Epoch 10 | Batch 150/469 | Loss: 0.6799\n",
      "   Epoch 10 | Batch 300/469 | Loss: 0.6190\n",
      "   Epoch 10 | Batch 450/469 | Loss: 0.5780\n",
      "----------------------------------------\n",
      " Epoch 10 Summary:\n",
      "   Time     : 143.6s\n",
      "   Train Acc: 79.25% | Test Acc: 78.61%\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "\n",
    "# 1. SETUP DEVICE (Detect GPU)\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\" Found GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        return torch.device(\"cuda\")\n",
    "    else:\n",
    "        print(\" GPU not found. Falling back to CPU.\")\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "\n",
    "# 2. THE DISTANCE-BIASED ATTENTION LAYER\n",
    "\n",
    "class DistanceBiasedAttention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Standard Linear Projections for Query, Key, Value\n",
    "        self.Q = nn.Linear(d_model, d_model)\n",
    "        self.K = nn.Linear(d_model, d_model)\n",
    "        self.V = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Learnable distance penalty weight\n",
    "        # Starts at 0.1, allowing the model to tune how much it cares about distance\n",
    "        self.distance_weight = nn.Parameter(torch.tensor(0.1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, seq_len, _ = x.shape\n",
    "        \n",
    "        q = self.Q(x)\n",
    "        k = self.K(x)\n",
    "        v = self.V(x)\n",
    "        \n",
    "        # Standard Scaled Dot-Product: (Q * K^T) / sqrt(d_model)\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.d_model ** 0.5)\n",
    "        \n",
    "        # --- THE DISTANCE PENALTY ---\n",
    "        # 1. Create a grid of sequence indices: [0, 1, 2, ..., seq_len-1]\n",
    "        indices = torch.arange(seq_len, device=x.device)\n",
    "        \n",
    "        # 2. Calculate absolute distance between all pairs of indices\n",
    "        # distance_matrix[i, j] = |i - j|\n",
    "        distance_matrix = torch.abs(indices.unsqueeze(0) - indices.unsqueeze(1))\n",
    "        \n",
    "        # 3. Apply the penalty to the scores\n",
    "        # Tokens far apart get a large deduction, focusing attention locally\n",
    "        scores = scores - (distance_matrix * self.distance_weight)\n",
    "        \n",
    "        # Softmax to get attention probabilities, then multiply by Values\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attn_weights, v)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "# 3. THE TRANSFORMER ARCHITECTURE\n",
    "\n",
    "class MNISTTransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Map a single pixel value (grayscale) to a 64-dimensional embedding\n",
    "        self.embedding = nn.Linear(1, 64) \n",
    "        \n",
    "        # Transformer Block 1\n",
    "        self.attn1 = DistanceBiasedAttention(d_model=64)\n",
    "        self.norm1a = nn.LayerNorm(64)\n",
    "        self.ffn1 = nn.Sequential(\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64)\n",
    "        )\n",
    "        self.norm1b = nn.LayerNorm(64)\n",
    "        \n",
    "        # Transformer Block 2\n",
    "        self.attn2 = DistanceBiasedAttention(d_model=64)\n",
    "        self.norm2a = nn.LayerNorm(64)\n",
    "        self.ffn2 = nn.Sequential(\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64)\n",
    "        )\n",
    "        self.norm2b = nn.LayerNorm(64)\n",
    "        \n",
    "        # Final classification head for digits 0-9\n",
    "        self.classifier = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input shape: [Batch, Channels=1, Height=28, Width=28]\n",
    "        b, c, h, w = x.shape\n",
    "        \n",
    "        # Flatten image into a sequence of 784 pixels: [Batch, 784, 1]\n",
    "        x = x.view(b, h*w, 1) \n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Block 1 (Attention -> Add & Norm -> FFN -> Add & Norm)\n",
    "        attn_out = self.attn1(x)\n",
    "        x = self.norm1a(x + attn_out)\n",
    "        ffn_out = self.ffn1(x)\n",
    "        x = self.norm1b(x + ffn_out)\n",
    "\n",
    "        # Block 2\n",
    "        attn_out = self.attn2(x)\n",
    "        x = self.norm2a(x + attn_out)\n",
    "        ffn_out = self.ffn2(x)\n",
    "        x = self.norm2b(x + ffn_out)\n",
    "        \n",
    "        # Global Average Pooling: Squash the 784 tokens into a single 64-dim vector\n",
    "        x = x.mean(dim=1) \n",
    "        \n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "# 4. DATA LOADING & TRAINING LOOP\n",
    "\n",
    "def main():\n",
    "    BATCH_SIZE = 128\n",
    "    EPOCHS = 10 # Kept short for quick testing\n",
    "    LEARNING_RATE = 0.001\n",
    "\n",
    "    print(\"\\n Downloading/Loading MNIST Data...\")\n",
    "    # Normalize pixel values to be between -1 and 1\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "    \n",
    "    train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model = MNISTTransformer().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(f\"\\n Starting Training on {device}...\\n\" + \"-\"*40)\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "            \n",
    "            # Print update every 150 batches\n",
    "            if batch_idx % 150 == 0:\n",
    "                print(f\"   Epoch {epoch+1} | Batch {batch_idx:03d}/{len(train_loader)} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # Evaluate on Test Set after each epoch\n",
    "        model.eval()\n",
    "        test_correct, test_total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                test_total += target.size(0)\n",
    "                test_correct += (predicted == target).sum().item()\n",
    "\n",
    "        train_acc = 100 * correct / total\n",
    "        test_acc = 100 * test_correct / test_total\n",
    "        epoch_time = time.time() - start_time\n",
    "        \n",
    "        print(\"-\" * 40)\n",
    "        print(f\" Epoch {epoch+1} Summary:\")\n",
    "        print(f\"   Time     : {epoch_time:.1f}s\")\n",
    "        print(f\"   Train Acc: {train_acc:.2f}% | Test Acc: {test_acc:.2f}%\")\n",
    "        print(\"-\" * 40 + \"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cfe9123-00e0-4dd9-8ed5-1165ccfc993e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def test_random_image(model, test_dataset):\n",
    "    print(\"\\n--- TESTING A RANDOM IMAGE ---\")\n",
    "    \n",
    "    # Put model in evaluation mode\n",
    "    model.eval() \n",
    "    \n",
    "    # Find out if the model is on CPU or GPU\n",
    "    device = next(model.parameters()).device \n",
    "    \n",
    "    # 1. Pick a random number between 0 and the length of the dataset (10,000)\n",
    "    random_index = random.randint(0, len(test_dataset) - 1)\n",
    "    \n",
    "    # 2. Grab that specific image and its true label\n",
    "    single_image, true_label = test_dataset[random_index]\n",
    "    \n",
    "    # 3. Reshape the image for our custom Transformer: [Batch=1, Sequence=784, Features=1]\n",
    "    flat_image = single_image.view(1, 784, 1).to(device)\n",
    "    \n",
    "    # 4. Pass it through the model to get the prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(flat_image)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        predicted_label = predicted.item()\n",
    "        \n",
    "    # 5. Print the results\n",
    "    print(f\"Image Index: #{random_index}\")\n",
    "    print(f\" The AI guessed: {predicted_label}\")\n",
    "    print(f\" The actual answer was: {true_label}\")\n",
    "    \n",
    "    # 6. Show the image!\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.imshow(single_image.squeeze(), cmap='gray')\n",
    "    \n",
    "    # Color the title green if correct, red if wrong\n",
    "    color = 'green' if predicted_label == true_label else 'red'\n",
    "    plt.title(f\"AI Guessed: {predicted_label} | Actual: {true_label}\", color=color)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b61271e-39e0-4ac8-ade0-4fc04f97fed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading one image for a quick test...\n",
      "\n",
      "Opening image window...\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ==========================================\n",
    "# 1. THE ARCHITECTURE\n",
    "# ==========================================\n",
    "class DistanceBiasedAttention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.Q = nn.Linear(d_model, d_model)\n",
    "        self.K = nn.Linear(d_model, d_model)\n",
    "        self.V = nn.Linear(d_model, d_model)\n",
    "        self.distance_weight = nn.Parameter(torch.tensor(0.1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Expects EXACTLY 3 dimensions: [Batch, SeqLen, Features]\n",
    "        B, seq_len, _ = x.shape\n",
    "        q, k, v = self.Q(x), self.K(x), self.V(x)\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.d_model ** 0.5)\n",
    "        \n",
    "        indices = torch.arange(seq_len, device=x.device)\n",
    "        distance_matrix = torch.abs(indices.unsqueeze(0) - indices.unsqueeze(1))\n",
    "        scores = scores - (distance_matrix * self.distance_weight)\n",
    "        \n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        return torch.matmul(attn_weights, v)\n",
    "\n",
    "class FlexibleTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(input_dim, 64) \n",
    "        self.attn1 = DistanceBiasedAttention(d_model=64)\n",
    "        self.norm1 = nn.LayerNorm(64)\n",
    "        self.classifier = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.norm1(x + self.attn1(x))\n",
    "        x = x.mean(dim=1) \n",
    "        return self.classifier(x)\n",
    "\n",
    "# ==========================================\n",
    "# 2. LOAD ONE IMAGE AND TEST (FIXED)\n",
    "# ==========================================\n",
    "print(\"Loading one image for a quick test...\")\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Load standard MNIST without modifying internals\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform) \n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True) \n",
    "\n",
    "# Initialize an \"untrained\" model\n",
    "model = FlexibleTransformer(input_dim=1, num_classes=10)\n",
    "model.eval()\n",
    "\n",
    "# Grab one batch (which contains 1 image)\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Get the true label\n",
    "true_label = labels[0].item()\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# THE FIX: Explicitly reshape the image to [1, 784, 1] \n",
    "# This guarantees exactly 3 dimensions for the Attention layer\n",
    "# -----------------------------------------------------\n",
    "flat_image = images.view(1, 784, 1)\n",
    "\n",
    "# Get prediction\n",
    "with torch.no_grad():\n",
    "    output = model(flat_image)\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    predicted_label = predicted.item()\n",
    "\n",
    "# Plot the result\n",
    "print(\"\\nOpening image window...\")\n",
    "plt.figure(figsize=(4,4))\n",
    "# Reshape the original image back to 28x28 just for plotting\n",
    "plt.imshow(images[0].squeeze(), cmap='gray')\n",
    "plt.title(f\"AI Guessed: {predicted_label} (Random)\\nActual: {true_label}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b381f70b-9973-4df5-82a0-64d982ef35d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading one image for a quick test...\n",
      "\n",
      "Opening image window...\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ==========================================\n",
    "# 1. THE ARCHITECTURE\n",
    "# ==========================================\n",
    "class DistanceBiasedAttention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.Q = nn.Linear(d_model, d_model)\n",
    "        self.K = nn.Linear(d_model, d_model)\n",
    "        self.V = nn.Linear(d_model, d_model)\n",
    "        self.distance_weight = nn.Parameter(torch.tensor(0.1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Expects EXACTLY 3 dimensions: [Batch, SeqLen, Features]\n",
    "        B, seq_len, _ = x.shape\n",
    "        q, k, v = self.Q(x), self.K(x), self.V(x)\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.d_model ** 0.5)\n",
    "        \n",
    "        indices = torch.arange(seq_len, device=x.device)\n",
    "        distance_matrix = torch.abs(indices.unsqueeze(0) - indices.unsqueeze(1))\n",
    "        scores = scores - (distance_matrix * self.distance_weight)\n",
    "        \n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        return torch.matmul(attn_weights, v)\n",
    "\n",
    "class FlexibleTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(input_dim, 64) \n",
    "        self.attn1 = DistanceBiasedAttention(d_model=64)\n",
    "        self.norm1 = nn.LayerNorm(64)\n",
    "        self.classifier = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.norm1(x + self.attn1(x))\n",
    "        x = x.mean(dim=1) \n",
    "        return self.classifier(x)\n",
    "\n",
    "# ==========================================\n",
    "# 2. LOAD ONE IMAGE AND TEST (FIXED)\n",
    "# ==========================================\n",
    "print(\"Loading one image for a quick test...\")\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Load standard MNIST without modifying internals\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform) \n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True) \n",
    "\n",
    "# Initialize an \"untrained\" model\n",
    "model = FlexibleTransformer(input_dim=1, num_classes=10)\n",
    "model.eval()\n",
    "\n",
    "# Grab one batch (which contains 1 image)\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Get the true label\n",
    "true_label = labels[0].item()\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# THE FIX: Explicitly reshape the image to [1, 784, 1] \n",
    "# This guarantees exactly 3 dimensions for the Attention layer\n",
    "# -----------------------------------------------------\n",
    "flat_image = images.view(1, 784, 1)\n",
    "\n",
    "# Get prediction\n",
    "with torch.no_grad():\n",
    "    output = model(flat_image)\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    predicted_label = predicted.item()\n",
    "\n",
    "# Plot the result\n",
    "print(\"\\nOpening image window...\")\n",
    "plt.figure(figsize=(4,4))\n",
    "# Reshape the original image back to 28x28 just for plotting\n",
    "plt.imshow(images[0].squeeze(), cmap='gray')\n",
    "plt.title(f\"AI Guessed: {predicted_label} (Random)\\nActual: {true_label}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "718f5937-a2c1-43c9-8c96-1350ec949d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading one image for a quick test...\n",
      "\n",
      "Opening image window...\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ==========================================\n",
    "# 1. THE ARCHITECTURE\n",
    "# ==========================================\n",
    "class DistanceBiasedAttention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.Q = nn.Linear(d_model, d_model)\n",
    "        self.K = nn.Linear(d_model, d_model)\n",
    "        self.V = nn.Linear(d_model, d_model)\n",
    "        self.distance_weight = nn.Parameter(torch.tensor(0.1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Expects EXACTLY 3 dimensions: [Batch, SeqLen, Features]\n",
    "        B, seq_len, _ = x.shape\n",
    "        q, k, v = self.Q(x), self.K(x), self.V(x)\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.d_model ** 0.5)\n",
    "        \n",
    "        indices = torch.arange(seq_len, device=x.device)\n",
    "        distance_matrix = torch.abs(indices.unsqueeze(0) - indices.unsqueeze(1))\n",
    "        scores = scores - (distance_matrix * self.distance_weight)\n",
    "        \n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        return torch.matmul(attn_weights, v)\n",
    "\n",
    "class FlexibleTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(input_dim, 64) \n",
    "        self.attn1 = DistanceBiasedAttention(d_model=64)\n",
    "        self.norm1 = nn.LayerNorm(64)\n",
    "        self.classifier = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.norm1(x + self.attn1(x))\n",
    "        x = x.mean(dim=1) \n",
    "        return self.classifier(x)\n",
    "\n",
    "# ==========================================\n",
    "# 2. LOAD ONE IMAGE AND TEST (FIXED)\n",
    "# ==========================================\n",
    "print(\"Loading one image for a quick test...\")\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Load standard MNIST without modifying internals\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform) \n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True) \n",
    "\n",
    "# Initialize an \"untrained\" model\n",
    "model = FlexibleTransformer(input_dim=1, num_classes=10)\n",
    "model.eval()\n",
    "\n",
    "# Grab one batch (which contains 1 image)\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Get the true label\n",
    "true_label = labels[0].item()\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# THE FIX: Explicitly reshape the image to [1, 784, 1] \n",
    "# This guarantees exactly 3 dimensions for the Attention layer\n",
    "# -----------------------------------------------------\n",
    "flat_image = images.view(1, 784, 1)\n",
    "\n",
    "# Get prediction\n",
    "with torch.no_grad():\n",
    "    output = model(flat_image)\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    predicted_label = predicted.item()\n",
    "\n",
    "# Plot the result\n",
    "print(\"\\nOpening image window...\")\n",
    "plt.figure(figsize=(4,4))\n",
    "# Reshape the original image back to 28x28 just for plotting\n",
    "plt.imshow(images[0].squeeze(), cmap='gray')\n",
    "plt.title(f\"AI Guessed: {predicted_label} (Random)\\nActual: {true_label}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900d5d49-cfb1-462c-84d5-40162f7956f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow GPU (D Drive)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
